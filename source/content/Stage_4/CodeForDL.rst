#############
DL 时代的编程
#############

*DL is eating software* [DLES]_  已经来临。 开发与测试的角色也开始起来变化。在传统软件时代，开发重于测试，需要大量的编程技巧。而测试对于能力的要求比较低。 在深度学习时代，传统的代码只是用来处理数据的预处理。同时有着大量的库的存在，而不需要太多的编程技能。 但是对于专业的要求，如何测试与训练模型是重点。而这个正是传统软件中测试的角色。

测试的核心那就是测试数据的设计。 原来的那些数据分类，边界值，极限值,默认值之类都用在DL数据的预处理上。DL时代编码主要是两块，数据的处理，然后网络的拓扑的设计。

并且随着的时间发展，DL 会把现在三十年来所有的software stack几乎会重塑一遍。


现在的DL就是一个最好约束解求过程。例如你找到一堆的数据，并且已经知道了一些约束当做cost函数，训练的过程，就是一个
通用的迭代解方程的过程。



未来的编程基础
=============

#. 向量与各种维度的转换。

   - 面向对象的设计模式都被向量张量所取代。
   - 以后最常见的运算之一，那就是 向量/张量 dim 的reshape变换，然后就是矩阵乘了。array.reshape,stack/cat.
   - 以后也不用为各种变量命名而头疼了，都被转化成了下标值 1，2，3，4。。。
   - group,sum，降维   join升维。 
   - 各种中间变量，可能就是临时网络例如,h2可以当做临时网络用于它用。
   
     .. code-block::bash
        
        def forward(self,x):
            h1 = self.module(x)
            h2 = self.module(x)
            return (h1,h2)

#. 矩阵乘，包含最重要的两种运算 加法与乘法。你要学会矩阵乘 用表示 各种模型关系。
   未来大部分的if,else 等等流控都会被 矩阵乘 + softmax 所取代。if,else 更加多的是coss函数以及各种约束来取代。
   进化的本质是就是复制
#. 测度理论，各种度量的模型的发展，最基本的那与范数 概率论.
#. 可视化, 未来软件的最基本调试方法。 
#. 优化理论。

#. 函数的复用，未来就会变成计算流图+model参数集吧。更大的库就是 Docker + volume 这样的数据流传递。

#. 如何实现结构化。那就是输入端把这个变量组合拼接进行，然后输出端将其分解出来，然后在两者之间
   建立一个测量函数(cost),并且确定其优化的方向。然后让优化器是迭代。

   - DSSPN 就是一个很好的把网络结构 tree 化的一个不错的例子。 https://arxiv.org/pdf/1803.06067.pdf


虽然都是基于python来的，但是一定注意其是完全独立一套。
变量，基本单位是tensor,所有的基本计算都是经过重新封装完备的，只要你用的是这个包之内的var,以及基本表达式操作。就应该是完备支持微分的，
如果缺一个基本函数，就要自己扩展，一定要注意其是否出圈了。 尽可能这个系统的原语。 并且知道如何扩展原语。

同时对于reshape重排，最后存储方式，并且没有改变，只是读写方式改变了。所谓的axios 你理解为沿哪一个轴进行for沿线，默认理解 0->沿x轴，1->沿y轴，2自动。 其实就是多级for循环怎么写了，默认的顺序就是这个轴0->轴n,然后每一个轴上从0->N. 
同时对对于矩阵乘的对于高维的高亮，只要看最后两维。 
矩阵乘法形式，那就是方组程写法，并且在numpy中，平方的写法，可以inter.product,也就是点对点乘，outer.product 就是一个排列组合。broadcast就是自动一维扩展到最大。 就是为简化矩阵乘以标量，简化表达。 

并且要学会用bool数组来代替，if/else的这种写法。 并且也有where,select,piecewise等专用符号计算方法。而少原始for loop if 等等。

包括后面各种层级network能是通过数学计算来替代分支。

在分层模块化的编程语言环境里，采用的微内核型，直接使用它的直接下级来表达当前这么，不要进行跨级调用，这样才能方便每一级的优化与扩展。




张量式建模
=========

以前设计一个结构体 

.. code-block:: c

   struct st {
      int a,
      int b,
      float 3,
      string name,
   }
   
现在都变成了张量式只有浮点数了。

.. code-block:: c
   array str[n1][n2][n3][n....] = [[][][][]]
   
所有的数据经过归一正则式都可以变成一个浮点数。 

* 对于任何一个属性找到一种编码，并且知道其最大值与最小值，然后再简单 :math:`norm_x=(x-min)/(max-min)` 
* 固定的难度一定要再加上一层scale,这个scale化，最直接的体现那是index. 
* 点线面体之间的相互转化。 
* 各种的group操作，sum,min,max,std,可以看做是一种降维的操作。
* 各种的stack,repeat都是一种数据结构扩展。
* 直接利用group计算 + bool向量以及矩阵化计算。 
* 对于各种矩阵计算，如何设计其每一步计算的正确性，那就要求每一个函数都要写测试函数，要充分其0，1，以及极值等值来进行验证，并且还要保证其精度. 不然由于积累误差与截断效应，就会大相径庭。 
   
   * 对于training的，如果初值是固定的，那么直接看cost 0,时cost. 

各种计算，一定要注意哪些向量式表达，哪些是标量的表达，哪些是矩阵式表达，并且直接用专用运算符表达，或者直接用 * 的时候，一定要知道其context,并且这个时候用的什么哪种乘法。


基本的表示符号
=============

第一步是向量化你的基本参数，然后在此基础上进一步延伸，这样不断产生了高维。由于我们一般人习惯了表的按行结构。而在向量里，我们习惯按列来写。 也就用表示结构，用例来表示scale up. 主要是看选择左乘，还是后乘。 也就是那个矩阵方程 :math:`y=&(WX+b)`. 还是 :math:`Y=&(XW+b)`.  如果单个的X主要转置。一般采用左乘 以及列式向量这样方便书写。

这个就像选用左右手定则之后，就像在这个系统里保持一致。并且， W,b,Z,A 之间的关系。
激活函数是自计算，而WX+b 这个是连接计算，本身就是矩阵乘的形式。

.. math::

   A^[0]= X
   Z^[n] = W^[n]A^[n-1] +b^[n]
   A^[n] = G(Z^[n])
   
.. code-block:: c
    x1 = [1,
          2,
          3,]
    
    X = [x1,x2,x3,....]

    Z = [X1,X2,X3, ....]
    
现在整体的计算， 同时多个样本的计算，同时多个网络的计算。 loss函数一个单个sample的距离，而cost所有sample的loss函数之和。 

我们可以把已经有的传统知识都当做约束 理论与现实的差距为什么不用DL来连接呢
==========================================================================

适配的过程的经验过程，不正是DL发挥威力的过程。 DL的核心本质 大数据，就不正是快速获得经验.
任何事情，都分都分已知部分与未知部分。结合神经网络好处，我设计已经部分，例如拓扑的结构或者cost的函数的设计上，把未知的部分扔给神经网络通过训练来得到。 这样就可以不断的探索求知了，当然更进一步，我们也可以像AlphaZero一样，从零开始训练，是不是得到更好的知识，从而打破我们原来所固有的经验限制。

对于大的DL系统，每一个模块可以采用联合训练的模式，也可以采用分块分步训练的模式。

toolchain的加速
===============

由于硬件迭代速度也越来越快，但是相应的toolchain的更新速度，却跟不上.因为toolchain的充分利用先的硬件也是需要优化与时间，如何加快这种迭代。 一种像LLVM，TVM,实现中间层的隔离，解决了一部分应用层加速的问题，但是toolchain的自身的更新问题仍然没有解决。
http://tvmlang.org/



reference
=========

.. [DLES] https://petewarden.com/2017/11/13/deep-learning-is-eating-software/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Deep%20Learning%20Weekly
.. [吴恩达笔记] https://github.com/fengdu78/deeplearning_ai_books
