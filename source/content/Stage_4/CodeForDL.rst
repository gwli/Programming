#############
DL 时代的编程
#############

*DL is eating software* [DLES]_  已经来临。 开发与测试的角色也开始起来变化。在传统软件时代，开发重于测试，需要大量的编程技巧。而测试对于能力的要求比较低。 在深度学习时代，传统的代码只是用来处理数据的预处理。同时有着大量的库的存在，而不需要太多的编程技能。 但是对于专业的要求，如何测试与训练模型是重点。而这个正是传统软件中测试的角色。

测试的核心那就是测试数据的设计。 原来的那些数据分类，边界值，极限值,默认值之类都用在DL数据的预处理上。DL时代编码主要是两块，数据的处理，然后网络的拓扑的设计。

并且随着的时间发展，DL 会把现在三十年来所有的software stack几乎会重塑一遍。


现在的DL就是一个最好约束解求过程。例如你找到一堆的数据，并且已经知道了一些约束当做cost函数，训练的过程，就是一个
通用的迭代解方程的过程。

未来的编程基础
==============

#. 向量与各种维度的转换。

   - 面向对象的设计模式都被向量张量所取代。
   - 以后最常见的运算之一，那就是 向量/张量 dim 的reshape变换，然后就是矩阵乘了。array.reshape,stack/cat.
   - 以后也不用为各种变量命名而头疼了，都被转化成了下标值 1，2，3，4。。。
   - group,sum，降维   join升维。 
   - 各种中间变量，可能就是临时网络例如,h2可以当做临时网络用于它用。
   
     .. code-block::bash
        
        def forward(self,x):
            h1 = self.module(x)
            h2 = self.module(x)
            return (h1,h2)

#. 矩阵乘，包含最重要的两种运算 加法与乘法。你要学会矩阵乘 用表示 各种模型关系。
   未来大部分的if,else 等等流控都会被 矩阵乘 + softmax 所取代。if,else 更加多的是coss函数以及各种约束来取代。
   进化的本质是就是复制
#. 测度理论，各种度量的模型的发展，最基本的那与范数 概率论.
#. 可视化, 未来软件的最基本调试方法。 
#. 优化理论。

#. 函数的复用，未来就会变成计算流图+model参数集吧。更大的库就是 Docker + volume 这样的数据流传递。

#. 如何实现结构化。那就是输入端把这个变量组合拼接进行，然后输出端将其分解出来，然后在两者之间
   建立一个测量函数(cost),并且确定其优化的方向。然后让优化器是迭代。

   - DSSPN 就是一个很好的把网络结构 tree 化的一个不错的例子。 https://arxiv.org/pdf/1803.06067.pdf


虽然都是基于python来的，但是一定注意其是完全独立一套。
变量，基本单位是tensor,所有的基本计算都是经过重新封装完备的，只要你用的是这个包之内的var,以及基本表达式操作。就应该是完备支持微分的，
如果缺一个基本函数，就要自己扩展，一定要注意其是否出圈了。 尽可能这个系统的原语。 并且知道如何扩展原语。

同时对于reshape重排，最后存储方式，并且没有改变，只是读写方式改变了。所谓的axios 你理解为沿哪一个轴进行for沿线，默认理解 0->沿x轴，1->沿y轴，2自动。 其实就是多级for循环怎么写了，默认的顺序就是这个轴0->轴n,然后每一个轴上从0->N. 
同时对对于矩阵乘的对于高维的高亮，只要看最后两维。 
矩阵乘法形式，那就是方组程写法，并且在numpy中，平方的写法，可以inter.product,也就是点对点乘，outer.product 就是一个排列组合。broadcast就是自动一维扩展到最大。 就是为简化矩阵乘以标量，简化表达。 

并且要学会用bool数组来代替，if/else的这种写法。 并且也有where,select,piecewise等专用符号计算方法。而少原始for loop if 等等。

包括后面各种层级network能是通过数学计算来替代分支。

在分层模块化的编程语言环境里，采用的微内核型，直接使用它的直接下级来表达当前这么，不要进行跨级调用，这样才能方便每一级的优化与扩展。

要学会建模，各种数据建模成张量，


我们可以把已经有的传统知识都当做约束 理论与现实的差距为什么不用DL来连接呢
==========================================================================

适配的过程的经验过程，不正是DL发挥威力的过程。 DL的核心本质 大数据，就不正是快速获得经验.
任何事情，都分都分已知部分与未知部分。结合神经网络好处，我设计已经部分，例如拓扑的结构或者cost的函数的设计上，把未知的部分扔给神经网络通过训练来得到。 这样就可以不断的探索求知了，当然更进一步，我们也可以像AlphaZero一样，从零开始训练，是不是得到更好的知识，从而打破我们原来所固有的经验限制。

对于大的DL系统，每一个模块可以采用联合训练的模式，也可以采用分块分步训练的模式。

toolchain的加速
===============

由于硬件迭代速度也越来越快，但是相应的toolchain的更新速度，却跟不上.因为toolchain的充分利用先的硬件也是需要优化与时间，如何加快这种迭代。 一种像LLVM，TVM,实现中间层的隔离，解决了一部分应用层加速的问题，但是toolchain的自身的更新问题仍然没有解决。
http://tvmlang.org/



reference
=========

.. [DLES] https://petewarden.com/2017/11/13/deep-learning-is-eating-software/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Deep%20Learning%20Weekly

